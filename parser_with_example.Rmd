---
title: "Literary Naming"
author: "Суязова С.А."
date: "`r format(Sys.Date(), '%d %b %Y')`"
output: 
    html_document:
      keep_md: yes
      toc: true
      toc_float: true
---


```{r setup, include = FALSE}



# ОПЦИИ ОТЧЁТА -----------------------------------------------------------------

# параметры отображения блоков кода
knitr::opts_chunk$set(echo = F, message = F, warning = F)
knitr::opts_chunk$set(dev.args = list(png = list(type = 'cairo')))
# порядковый номер рисунка
fig_count <- 0




# ПАКЕТЫ -----------------------------------------------------------------------

library("knitr")
library("readtext")
library("dplyr")
library("data.table")
library("wordcloud2")
library("RColorBrewer")
library("htmlwidgets")
library("webshot")




# КОНСТАНТЫ --------------------------------------------------------------------

# количество слов с наибольшей встречаемостью
MAX_WORDS <- 5

# url текста книги для загрузки
#  The Project Gutenberg eBook of The Man Who Was Thursday: A Nightmare
URL_BOOK <- "https://www.gutenberg.org/cache/epub/1695/pg1695.txt"

# url таблицы со словарём
URL_VOCABULARY <- 
    "https://raw.githubusercontent.com/nalgeon/words/refs/heads/main/data/oxford-5k.csv"

# url файла со стоп-словами
URL_STOP <- 
    "https://raw.githubusercontent.com/stopwords-iso/stopwords-en/refs/heads/master/stopwords-en.txt"

# ядро для генератора случайных чисел
my_seed <- 50825




# ФУНКЦИИ ----------------------------------------------------------------------

# объединяем частоты некоторых форм одного и того же слова
eval(parse('./functions/uf_combine_word_forms_frequencies.R', 
           encoding = 'UTF-8'))

# генерируем словосочетание
eval(parse('./functions/uf_random_two_words_by_book.R', 
           encoding = 'UTF-8'))

```


```{r load-data}

# ПАРСЕР -----------------------------------------------------------------------



# 1. Загружаем данные из всех источников =======================================


# <<< Текст книги читаем с URL <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
raw_txt <- readtext(URL_BOOK)
# str(raw_txt)
#...............................................................................


# <<< Словарь стоп-слов английского языка читаем с URL <<<<<<<<<<<<<<<<<<<<<<<<<
#
stop_txt <- readLines(URL_STOP)
# str(stop_txt)
#
#...............................................................................


# <<< Словарь Оксфордский читаем с URL <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
#
DT_voc <- data.table(read.csv(URL_VOCABULARY)[, 1:3])
# str(DT_voc)
#
#...............................................................................


# <<< Вручную допиленная часть словаря под эту книгу из файла <<<<<<<<<<<<<<<<<<
#
df_add <- read.csv2('./reference/additional_voc.csv')
# str(df_add)
#
#...............................................................................


# <<< Таблица с окончаниями для подсчёта словоформ <<<<<<<<<<<<<<<<<<<<<<<<<<<<<
#
df_endings <- read.csv2('./reference/word_endings.csv')
# str(df_add)
#
#...............................................................................

```


## Идея парсера    

Во всевозможных веб-инструментах, требующих генерации большого количества уникальных имён (например, Google API, имена пользователей на reddit, капча), используются сочетания из случайных слов. Код этого репозитория генерирует случайное словосочетание из двух слов, оперируя существительными и прилагательными английского языка, которые чаще всего встречаются в заданном тексте. На данный момент это один текст: "The Man Who Was Thursday" за авторством Г.К.Честертона из Библиотеки Гутенберга [^1].   

Решение имеет два ограничения:   

1. В любом тексте будут встречаться формы одного и того же слова, например, единственное и множественное число, настоящее и прошедшее время. Для учёта повторов мы используем ограниченное количество форм слов английского языка: основа и варианты окончаний: `r paste0(paste0(c('', '«')[as.numeric(df_endings$pattern_1 != '') + 1], df_endings$pattern_1, c('', '»')[as.numeric(df_endings$pattern_1 != '') + 1]), c('', '/')[as.numeric(df_endings$pattern_1 != '') + 1], '«', df_endings$pattern_2, '»')`. При этом основой слова мы далее будем называть всё кроме перечисленных окончаний.       

1. Закономерно, что имена главных персонажей в большинстве книг будут встречаться чаще имён нарицательных. Из соображений улучшения однородности данных, генерировать случайные словосочетания целесообразно из имён нарицательных. Для разделения слов на имена собственные и нарицательные мы просто проверяем, входит ли слово в текст с большой или с маленькой буквы, либо и то, и другое. Это подход имеет минусы, в частности, имя нарицательное, встретившееся один раз, будет считаться именем собственным. Мы обходим этот момент, выбирая для генерации только слова с частотой встречаемости выше некоторого порога $k > 1$.    

По сути, 99% кода решает задачи очистки текстовых данных. Существенная часть этого процесса -- подсчёт частот в соответствии с ограничением №1, описанным выше.   


## Зависимости   

Код требует загрузки следующих пакетов:   

* ```knitr``` [^3]
* ```readtext``` [^4]
* ```dplyr``` [^5]
* ```data.table``` [^6]
* ```wordcloud2``` [^7]
* ```RColorBrewer``` [^8]
* ```htmlwidgets``` [^9]
* ```webshot``` [^10]


## Результаты  

```{r clean-data}

# 2. Чистим данные =============================================================


# 1. Стартовая очистка, цель - вектор слов с частотами #########################

# Оставляем только текст книги

#  * выкидываем начало:
#    до названия первой главы
search_pattern <- '(.*\n)(CHAPTER I.\nTHE TWO POETS OF SAFFRON PARK\n.*)'
replace_pattern <- '\\2'
clean_txt <- gsub(search_pattern, replace_pattern, raw_txt$text)

#  * выкидываем описалово лицензии проекта Гуттенберга: 
#    после заданной фразы
search_pattern <- '\\\\*\\\\*\\\\* END OF THE PROJECT GUTENBERG EBOOK THE MAN WHO WAS THURSDAY: A NIGHTMARE \\\\*\\\\*\\\\*.*'
replace_pattern <- ''
clean_txt <- gsub(search_pattern, replace_pattern, clean_txt)


# Разбиваем текст построчно
clean_txt <- strsplit(clean_txt, '\n')[[1]]


# Убираем пустые строки
clean_txt <- clean_txt[clean_txt != '']


# Убираем названия глав
chapter_num <- c(grep('CHAPTER ', clean_txt), grep('CHAPTER ', clean_txt) + 1)
clean_txt <- clean_txt[!(1:length(clean_txt) %in% chapter_num)]


# Убираем знаки препинания
clean_txt <- sapply(clean_txt, function(x){
    gsub('[[:punct:]]', '', x)
    }) %>% unname %>% unlist


# Разбиваем на отдельные слова
clean_txt <- strsplit(clean_txt, ' ') %>% unlist %>% unname
tbl <- table(clean_txt)
DT <- data.table(word = names(tbl), freq = as.vector(tbl))


# Убираем пустоты
DT <- DT[!(DT$word == ''), ]


# Убираем числа
DT <- DT[!grepl('\\d', DT$word), ]


# 2. Продвинутая очистка: строчные/прописные буквы в начале слов ###############

# работаем с заглавными буквами
low_reg <- tolower(DT$word)

# эти слова начинаются с большой буквы
capital <- which(!(low_reg == DT$word))

# надо проверить, есть ли варианты написания со строчной
lower_match <- sapply(capital, function(x){
    which(DT$word == tolower(DT$word[x]))
})

# прибавляем к частотам строчных частоты заглавных
DT[lower_match[sapply(lower_match, length) == 1] %>% unlist, 'freq'] <- 
    DT[lower_match[sapply(lower_match, length) == 1] %>% unlist, freq] + 
    DT[capital[sapply(lower_match, length) == 1], freq]

# убираем заглавные
DT <- DT[!capital[sapply(lower_match, length) == 1], ]

# исправляем 'I' на 'i'
DT[DT$word == 'I', 'word'] <- 'i'


# 3. Совмещаем со словарями ####################################################

# убираем стоп-слова
DT <- DT %>% filter(!(DT$word %in% stop_txt))

# сортировка слов по алфавиту критически важна
DT <- arrange(DT, word)

# совмещаем простые словоформы
#  функция запускает цикл по всем вариантам окончаний и обновляет таблицу данных
DT <- uf_combine_word_forms_frequencies(DT, df_endings)

# сортировка по убыванию частот
DT <- arrange(DT, -freq)

# написание строчными буквами как ключ для поиска по словарям
DT$word_key <- tolower(DT$word)

```


```{r figure-01}

fig_count <- fig_count + 1

# >>> Рисунок 1 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# рисуем все слова с частотой не менее 20
fig_01 <- wordcloud2(data = DT[DT$freq >= 20, ])
#
# ..............................................................................

flnm <- paste0('./plots/plot_', sprintf("%02d", fig_count))
saveWidget(fig_01, paste0(flnm, '.html'), selfcontained = TRUE)

tmp <- webshot(url = paste0(flnm,'.html'), file = paste0(flnm,'.png'), 
               delay = 7, vwidth = 800, vheight = 400)

```

После предварительной очистки от стоп-слов в наборе данных осталось `r format(nrow(DT), big.mark = ' ')` слов. На рисунке ниже показано облако слов с частотой не менее 20 до соотнесения с частями речи. Чаще всего встречается "Syme" -- имя главного героя книги. "Bull", "Gregory", "Dr" также относятся к именам персонажей.   

![](./plots/plot_01.png)

Рисунок `r fig_count`. Облако слов по книге "The Man Who Was Thursday" с частотой не менее 20.   

Чтобы поставить каждому слову в соответствие часть речи, мы использовали таблицу по Оксфордскому словарю [^2]. Словарная таблица содержит только имена нарицательные, поэтому имена персонажей отфильтровываются автоматически (кроме Sunday, которое мы оставим в итоговом наборе). Некоторым нарицательным существительным и прилагательным из книги также не нашлось соответствия в этом словаре (например, anarchist, feet), и части речи для них пришлось добавить вручную.   


```{r calc-freq-01} 

# совмещаем с оксфордским словарём, чтобы узнать часть речи
words_to_shuffle <- merge(DT, DT_voc[, -2], 
                          by.x = 'word_key', by.y = 'word') 

# добавляем часть словаря, составленную вручную,
#  оставляем существительные и прилагательные с частотой не менее MAX_WORDS,
#  сортируем,
#  добавляем пустой столбец под цвета по градиенту
words_to_shuffle <- rbind(words_to_shuffle, df_add) %>% 
    filter(pos %in% c('noun', 'adjective'), freq >= MAX_WORDS) %>%
    arrange(pos, freq) %>% mutate(clrs = '')

# сколько существительных и прилагательных в итоге осталось
tbl <- words_to_shuffle %>% count(pos)

```


После того как мы объединили словоформы, осталось `r nrow(words_to_shuffle)` слова, из них `r tbl[tbl$pos == 'noun', 'n']` существительных (см. таблицу ниже).   

Таблица 1. -- Существительные и прилагательные в итоговом наборе слов ```words_to_shuffle```    

```{r table-01}

# таблица 1 
kable(tbl, row.names = F,
      col.names = c('Часть речи', 'Частота'))

```


```{r calc-freq-02}

# кодируем цвета по градиенту в столбец clrs: 
#  прилагательные синие, существительные красные
bluecols <- brewer.pal(5, 'Blues')
clrs_adj <- colorRampPalette(bluecols)

words_to_shuffle[grepl('adjective', words_to_shuffle$pos), 'clrs'] <- 
    clrs_adj(sum(grepl('adjective', words_to_shuffle$pos)))

redcols <- brewer.pal(5, 'Reds')
clrs_noun <- colorRampPalette(redcols)

words_to_shuffle[grepl('noun', words_to_shuffle$pos), 'clrs'] <- 
    clrs_noun(sum(grepl('noun', words_to_shuffle$pos)))

# сортировка по алфавиту (по частотам облако выходит странное),
#  плюс столбец с индексом для случайной выборки
words_to_shuffle <- words_to_shuffle[, -1] %>% arrange(word) %>% 
    mutate(id = 1:dim(words_to_shuffle)[1])
```


Облако слов для итогового набора для создания случайных словосочетаний показано на втором рисунке. Градиент цвета пропорционален частоте встречаемости слова.   


```{r figure-02}

fig_count <- fig_count + 1

# >>> Рисунок 2 >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
#
fig_02 <- wordcloud2(data = words_to_shuffle, color = words_to_shuffle$clrs)
#
# ..............................................................................

flnm <- paste0('./plots/plot_', sprintf("%02d", fig_count))
saveWidget(fig_02, paste0(flnm, '.html'), selfcontained = TRUE)

tmp <- webshot(url = paste0(flnm,'.html'), file = paste0(flnm,'.png'), 
               delay = 2)

```

![](./plots/plot_02.png)

Рисунок `r fig_count`. Облако слов по книге для создания словосочетаний: красным цветом показаны существительные, синим - прилагательные.   

Функция ```uf_random_two_words_by_book()``` выбирает из итогового набора данных прилагательное и существительное случайным образом. Пример использования:    


```{r res-01, echo=T}

# 4. Наконец составляем собственно случайное словосочетание ####################

uf_random_two_words_by_book(words_to_shuffle, my_seed)

```


В таблице показано ещё несколько примеров словосочетаний на базе книги.   


```{r}

# ядро генератора случайных чисел
rnd_seed <- ceiling(rnorm(m = as.numeric(format(Sys.time(), "%S")), 
                          s = as.numeric(format(Sys.time(), "%M")), 
                          1:10) * 10)

tbl <- data.table(seed = rnd_seed, 
           two_words = sapply(rnd_seed, function(x){ 
               uf_random_two_words_by_book(words_to_shuffle, x)
               })) %>% unique %>% arrange(two_words)

```


Таблица 2. -- Примеры словосочетаний и соответсвующие ядра для генератора случайных чисел    

```{r table-02}

# таблица 2 
kable(tbl, row.names = F,
      col.names = c('Ядро генератора случайных чисел', 'Словосочетание'))

```


## Планы   

🔸 📋 Найти более полный словарь для определения частей речи.    

🔸 📚 Добавить другие книги из Библиотеки Гуттенберга.   



[^1]: Project Gutenberg. URL: [https://www.gutenberg.org/](https://www.gutenberg.org/).   

[^2]: Репозиторий Stopwords ISO на github.com. URL: [https://github.com/stopwords-iso/stopwords-iso](https://github.com/stopwords-iso/stopwords-iso).  

[^3]: Xie Y (2025). _knitr: A General-Purpose Package for Dynamic Report
  Generation in R_. R package version 1.50, <https://yihui.org/knitr/>.    
  
[^4]: Benoit K, Obeng A (2025). _readtext: Import and Handling for Plain and
  Formatted Text Files_. doi:10.32614/CRAN.package.readtext
  <https://doi.org/10.32614/CRAN.package.readtext>, R package version 0.92.1,
  <https://CRAN.R-project.org/package=readtext>.    
  
[^5]: Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A
  Grammar of Data Manipulation_. doi:10.32614/CRAN.package.dplyr
  <https://doi.org/10.32614/CRAN.package.dplyr>, R package version 1.1.4,
  <https://CRAN.R-project.org/package=dplyr>.    
  
[^6]: Barrett T, Dowle M, Srinivasan A, Gorecki J, Chirico M, Hocking T,
  Schwendinger B, Krylov I (2025). _data.table: Extension of `data.frame`_.
  doi:10.32614/CRAN.package.data.table
  <https://doi.org/10.32614/CRAN.package.data.table>, R package version
  1.17.8, <https://CRAN.R-project.org/package=data.table>.    
  
[^7]: Lang D, Chien G (2018). _wordcloud2: Create Word Cloud by 'htmlwidget'_.
  doi:10.32614/CRAN.package.wordcloud2
  <https://doi.org/10.32614/CRAN.package.wordcloud2>, R package version
  0.2.1, <https://CRAN.R-project.org/package=wordcloud2>.    
  
[^8]: Neuwirth E (2022). _RColorBrewer: ColorBrewer Palettes_.
  doi:10.32614/CRAN.package.RColorBrewer
  <https://doi.org/10.32614/CRAN.package.RColorBrewer>, R package version
  1.1-3, <https://CRAN.R-project.org/package=RColorBrewer>.    
  
[^9]: Vaidyanathan R, Xie Y, Allaire J, Cheng J, Sievert C, Russell K (2023).
  _htmlwidgets: HTML Widgets for R_. doi:10.32614/CRAN.package.htmlwidgets
  <https://doi.org/10.32614/CRAN.package.htmlwidgets>, R package version
  1.6.4, <https://CRAN.R-project.org/package=htmlwidgets>.    
  
[^10]: Chang W (2023). _webshot: Take Screenshots of Web Pages_.
  doi:10.32614/CRAN.package.webshot
  <https://doi.org/10.32614/CRAN.package.webshot>, R package version 0.5.5,
  <https://CRAN.R-project.org/package=webshot>.    

[^11]: Репозиторий Oxford Word Lists пользователя nalgeon на github.com. URL: [https://github.com/nalgeon/words](https://github.com/nalgeon/words).   
